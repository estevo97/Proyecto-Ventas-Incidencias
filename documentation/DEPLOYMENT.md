# ğŸ“‹ GuÃ­a de Deployment: ETL Pipeline & Escalabilidad

## ğŸ¯ Resumen del Sistema

Este proyecto implementa un **pipeline ETL completo** para transformar datos raw en un esquema analytics optimizado para Power BI con escalabilidad infinita mediante DirectQuery.

### **Arquitectura:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL (Railway)              â”‚
â”‚   â”œâ”€â”€ Schema: public (RAW)          â”‚
â”‚   â”‚   â”œâ”€â”€ calendario_raw            â”‚
â”‚   â”‚   â”œâ”€â”€ productos_raw             â”‚
â”‚   â”‚   â”œâ”€â”€ rutas_raw                 â”‚
â”‚   â”‚   â”œâ”€â”€ ventas_raw (7,273 filas)  â”‚
â”‚   â”‚   â””â”€â”€ incidencias_raw (54)      â”‚
â”‚   â”‚                                  â”‚
â”‚   â””â”€â”€ Schema: analytics (LIMPIO)    â”‚
â”‚       â”œâ”€â”€ dim_calendario            â”‚
â”‚       â”œâ”€â”€ dim_productos             â”‚
â”‚       â”œâ”€â”€ dim_rutas                 â”‚
â”‚       â”œâ”€â”€ dim_tipo_incidencia       â”‚
â”‚       â”œâ”€â”€ fact_ventas               â”‚
â”‚       â”œâ”€â”€ fact_incidencias          â”‚
â”‚       â””â”€â”€ vw_ventas_incidencias... â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    ETL Pipeline (Python)
    - Extrae de 'public'
    - Transforma y enriquece
    - Carga en 'analytics'
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Power BI (DirectQuery)            â”‚
â”‚   - ConexiÃ³n a 'analytics'          â”‚
â”‚   - Dashboard escalable âˆ           â”‚
â”‚   - ActualizaciÃ³n automÃ¡tica        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Paso a Paso: Deployment

### **PASO 1: Verificar Schema Existente âœ…**

Ya tienes el schema de estrella en pgAdmin con:
- âœ… `calendario_raw`, `productos_raw`, `rutas_raw`
- âœ… `ventas_raw`, `incidencias_raw`
- âœ… Claves primarias y forÃ¡neas configuradas
- âœ… Restricciones de grano configuradas

**No necesitas ejecutar el script SQL 01_create_analytics_schema.sql**

Verifica que todo estÃ¡ correcto:
```sql
SELECT tc.table_name, kcu.column_name, ccu.table_name, ccu.column_name
FROM information_schema.table_constraints AS tc
JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name
JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name
WHERE tc.constraint_type = 'FOREIGN KEY'
ORDER BY tc.table_name;
```

---

### **PASO 2: Configurar ETL Pipeline**

**2.1. Actualizar credenciales en `etl_pipeline.py`:**

Edita la lÃ­nea 35:

```python
DB_CONFIG = {
    'host': 'hopper.proxy.rlwy.net',
    'port': 57761,
    'database': 'central_data',
    'user': 'postgres',
    'password': 'TU_PASSWORD_REAL'  # âš ï¸ CAMBIAR ESTO
}
```

**2.2. Instalar dependencias:**

```powershell
cd D:\OneDrive\Documentos\GitHub\Proyecto-Ventas-Incidencias
pip install -r requirements.txt
```

**2.3. Ejecutar ETL por primera vez:**

```powershell
python scripts/etl_pipeline.py
```

**Salida esperada:**
```
============================================================
ğŸš€ INICIANDO ETL PIPELINE
============================================================
âœ… ConexiÃ³n a PostgreSQL establecida
ğŸ“¥ Extrayendo datos de schema 'public' (raw)...
  âœ… calendario: 366 filas extraÃ­das
  âœ… productos: 6 filas extraÃ­das
  âœ… rutas: 8 filas extraÃ­das
  âœ… ventas: 7273 filas extraÃ­das
  âœ… incidencias: 54 filas extraÃ­das

ğŸ”„ FASE DE TRANSFORMACIÃ“N
------------------------------------------------------------
ğŸ”„ Transformando dim_calendario...
  âœ… Calendario transformado: 366 registros
...
âœ… ETL COMPLETADO EXITOSAMENTE
â±ï¸  DuraciÃ³n: 3.45 segundos
```

---

### **PASO 3: Crear Ãndices de OptimizaciÃ³n**

Ejecuta el script de optimizaciÃ³n en pgAdmin:

```bash
# Archivo: scripts/02_create_indexes.sql
```

**En pgAdmin:**
1. Query Tool â†’ Cargar `02_create_indexes.sql`
2. Ejecuta (F5)
3. Espera ~30 segundos

**Resultado esperado:**
- âœ… 20+ Ã­ndices creados en tus tablas (calendario, productos, rutas, ventas, incidencias)
- âœ… Tablas reorganizadas (CLUSTER)
- âœ… EstadÃ­sticas actualizadas (VACUUM ANALYZE)

**VerificaciÃ³n:**
```sql
-- Ver Ã­ndices creados en public
SELECT tablename, indexname 
FROM pg_indexes 
WHERE schemaname = 'public'
ORDER BY tablename;
```

---

### **PASO 4: Conectar Power BI**

#### **4.1. Configurar conexiÃ³n DirectQuery**

En Power BI Desktop:

1. **Get Data** â†’ **PostgreSQL database**
2. Ingresar credenciales:
   - **Server:** `hopper.proxy.rlwy.net:57761`
   - **Database:** `central_data`
3. **Seleccionar tablas del schema public:**
   - `calendario_raw`
   - `productos_raw`
   - `rutas_raw`
   - `ventas_raw`
   - `incidencias_raw`
4. En modo de conexiÃ³n: **DirectQuery** (no Import)
5. AutenticaciÃ³n:
   - **User:** `postgres`
   - **Password:** tu password de Railway

#### **4.2. Crear modelo de datos**

Crear relaciones:
   - `ventas_raw[fecha]` â†’ `calendario_raw[fecha]` (Many-to-One)
   - `ventas_raw[rutaid]` â†’ `rutas_raw[rutaid]` (Many-to-One)
   - `ventas_raw[productoid]` â†’ `productos_raw[productoid]` (Many-to-One)
   - `incidencias_raw[fecha]` â†’ `calendario_raw[fecha]` (Many-to-One)
   - `incidencias_raw[rutaid]` â†’ `rutas_raw[rutaid]` (Many-to-One)

#### **4.3. Actualizar visualizaciones**

Tu dashboard existente deberÃ­a funcionar automÃ¡ticamente con las nuevas relaciones e Ã­ndices optimizados.

---

### **PASO 5: AutomatizaciÃ³n (Opcional pero Recomendado)**

#### **OpciÃ³n A: GitHub Actions (Gratis, Cloud)**

Crear archivo `.github/workflows/etl_daily.yml`:

```yaml
name: ETL Pipeline Diario

on:
  schedule:
    - cron: '0 2 * * *'  # Ejecuta a las 2 AM UTC diariamente
  workflow_dispatch:  # Permite ejecuciÃ³n manual

jobs:
  run-etl:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout cÃ³digo
      uses: actions/checkout@v3
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Instalar dependencias
      run: |
        pip install pandas psycopg2-binary
    
    - name: Ejecutar ETL
      env:
        DB_PASSWORD: ${{ secrets.RAILWAY_DB_PASSWORD }}
      run: |
        python scripts/etl_pipeline.py
```

**Configurar secretos en GitHub:**
1. Repositorio â†’ Settings â†’ Secrets â†’ New secret
2. Nombre: `RAILWAY_DB_PASSWORD`
3. Valor: tu password de Railway

#### **OpciÃ³n B: Windows Task Scheduler (Local)**

1. Abrir Task Scheduler
2. Create Task:
   - **Name:** ETL Pipeline Ventas
   - **Trigger:** Daily at 2:00 AM
   - **Action:** Start a program
     - Program: `python`
     - Arguments: `D:\OneDrive\Documentos\GitHub\Proyecto-Ventas-Incidencias\scripts\etl_pipeline.py`
3. Save

---

## ğŸ“Š Uso Diario

### **Agregar nuevos datos:**

1. Inserta en tablas `*_raw` (public):
   ```sql
   INSERT INTO public.ventas_raw (...) VALUES (...);
   INSERT INTO public.incidencias_raw (...) VALUES (...);
   ```

2. Ejecuta ETL:
   ```powershell
   python scripts/etl_pipeline.py
   ```

3. Power BI se actualiza automÃ¡ticamente (DirectQuery)

### **Monitoring:**

```sql
-- Ver Ãºltima actualizaciÃ³n
SELECT MAX(fecha_carga) FROM analytics.fact_ventas;

-- Contar registros
SELECT 
    'ventas' as tabla, COUNT(*) as filas FROM analytics.fact_ventas
UNION ALL
SELECT 'incidencias', COUNT(*) FROM analytics.fact_incidencias;
```

---

## ğŸ”§ Troubleshooting

### **Error: "Permission denied for schema analytics"**
```sql
GRANT ALL ON SCHEMA analytics TO postgres;
GRANT ALL ON ALL TABLES IN SCHEMA analytics TO postgres;
```

### **Error: "Connection timed out"**
- Verifica que Railway DB estÃ© activa
- Revisa credenciales en `etl_pipeline.py`

### **Power BI lento:**
```sql
-- Re-ejecuta optimizaciÃ³n
\i scripts/02_create_indexes.sql
```

### **Ver logs del ETL:**
```powershell
cat etl_pipeline.log
```

---

## ğŸ“ˆ Escalabilidad Confirmada

| MÃ©trica | Valor Actual | LÃ­mite TeÃ³rico |
|---------|--------------|----------------|
| Filas en ventas | 7,273 | âˆ (DirectQuery) |
| Filas en incidencias | 54 | âˆ |
| TamaÃ±o .pbix | Cualquiera | No crece (DirectQuery) |
| Tiempo de refresco | InstantÃ¡neo | <2seg con Ã­ndices |
| Nuevos datos | Inserta + ETL | Sin lÃ­mite |

---

## âœ… Checklist Final

- [ ] Credenciales actualizadas en `etl_pipeline.py`
- [ ] Dependencias instaladas (`pip install -r requirements.txt`)
- [ ] ETL ejecutado exitosamente (python scripts/etl_pipeline.py)
- [ ] Ãndices creados en PostgreSQL (scripts/02_create_indexes.sql)
- [ ] Power BI conectado con DirectQuery
- [ ] Relaciones FK creadas en Power BI
- [ ] Dashboard funciona correctamente
- [ ] (Opcional) AutomatizaciÃ³n configurada
- [ ] (Opcional) DocumentaciÃ³n actualizada en README principal

---

## ğŸ“ PrÃ³ximos Pasos (Mejoras Futuras)

1. **Vistas materializadas** para reportes muy complejos
2. **Particionamiento por fecha** si creces a millones de filas
3. **CDC (Change Data Capture)** para ETL incremental
4. **Alertas** cuando hay incidencias crÃ­ticas
5. **Machine Learning** para predecir impacto de incidencias

---

**Â¡Tu sistema ahora escala infinitamente! ğŸš€**
